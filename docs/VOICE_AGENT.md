# Advanced Voice Agent System Documentation - Zerko Platform

## üìã Overview

The Zerko Voice Agent System represents a cutting-edge implementation of real-time voice-based interview technology. This sophisticated system seamlessly integrates browser-native Web Speech API with advanced AI processing, intelligent error handling, cross-browser compatibility, and robust fallback mechanisms to deliver a professional-grade interview experience across all major browsers and devices.

### Key Capabilities

- **Universal Browser Support**: Optimized for Chrome, Firefox, Safari, Edge, and Brave
- **Intelligent Speech Recognition**: Advanced processing with confidence scoring and quality metrics
- **Real-time Audio Processing**: Low-latency speech-to-text with noise suppression
- **Adaptive Error Recovery**: Multi-layered error handling with automatic fallback systems
- **Cross-Platform Compatibility**: Consistent experience across desktop and mobile devices
- **AI-Powered Conversation**: Integration with Google Gemini 2.5 Pro for intelligent responses

## üèóÔ∏è Architecture

### System Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    VOICE AGENT SYSTEM                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Browser Detection   ‚îÇ  ‚Üê Async detection with fallbacks
‚îÇ  - Chrome, Edge      ‚îÇ
‚îÇ  - Firefox, Safari   ‚îÇ
‚îÇ  - Brave (special)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SpeechRecognition    ‚îÇ  ‚Üê Browser-specific configuration
‚îÇ Manager              ‚îÇ
‚îÇ  - Instance creation ‚îÇ
‚îÇ  - Config management ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Speech Recognition  ‚îÇ  ‚Üê Web Speech API
‚îÇ  - Start/Stop        ‚îÇ
‚îÇ  - Result handling   ‚îÇ
‚îÇ  - Error management  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Silence Detection   ‚îÇ  ‚Üê Intelligent speech end detection
‚îÇ  - Timestamp tracking‚îÇ
‚îÇ  - Threshold checking‚îÇ
‚îÇ  - Auto-stop         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Session Transcript  ‚îÇ  ‚Üê Accumulate speech results
‚îÇ  - Interim results   ‚îÇ
‚îÇ  - Final results     ‚îÇ
‚îÇ  - Submit on end     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Error Handling      ‚îÇ  ‚Üê Comprehensive error recovery
‚îÇ  - Network retry     ‚îÇ
‚îÇ  - Permission errors ‚îÇ
‚îÇ  - Fallback mode     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîß Core Components

### 1. Browser Detection System


#### Implementation

```typescript
const getBrowserInfo = async () => {
  if (typeof window === 'undefined') {
    return { 
      isBrave: false, isChrome: false, isFirefox: false, 
      isSafari: false, isEdge: false, browserName: 'unknown'
    };
  }
  
  const userAgent = navigator.userAgent;
  
  // Detect Edge (check before Chrome)
  const isEdge = userAgent.includes('Edg/') || userAgent.includes('Edge/');
  
  // Detect Chrome (exclude Edge and Brave)
  const isChrome = userAgent.includes('Chrome') && !isEdge;
  
  // Detect Firefox
  const isFirefox = userAgent.includes('Firefox');
  
  // Detect Safari (exclude Chrome-based)
  const isSafari = /^((?!chrome|android).)*safari/i.test(userAgent);
  
  // Brave detection with multiple fallbacks
  let isBrave = false;
  
  // Method 1: Official Brave API (most reliable)
  if (window.brave && typeof window.brave.isBrave === 'function') {
    try {
      isBrave = await window.brave.isBrave();
      console.log('Brave detected via official API');
    } catch (e) {
      console.warn('Brave API detection failed:', e);
    }
  }
  
  // Method 2: User agent string
  if (!isBrave && userAgent.includes('Brave')) {
    isBrave = true;
    console.log('Brave detected via user agent');
  }
  
  // Method 3: Navigator property
  if (!isBrave && (navigator as any).brave !== undefined) {
    isBrave = true;
    console.log('Brave detected via navigator property');
  }
  
  const browserName = isBrave ? 'Brave' 
    : isEdge ? 'Edge'
    : isChrome ? 'Chrome'
    : isFirefox ? 'Firefox'
    : isSafari ? 'Safari'
    : 'unknown';
  
  return { isBrave, isChrome, isFirefox, isSafari, isEdge, browserName };
};
```

#### Detection Methods

| Method | Priority | Reliability | Notes |
|--------|----------|-------------|-------|
| Official Brave API | 1 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Most reliable, async |
| User Agent String | 2 | ‚≠ê‚≠ê‚≠ê | Can be spoofed |
| Navigator Property | 3 | ‚≠ê‚≠ê‚≠ê‚≠ê | Good fallback |

### 2. SpeechRecognitionManager Class

#### Purpose
Encapsulates all browser-specific speech recognition logic and configuration management.

#### Implementation

```typescript
class SpeechRecognitionManager {
  private recognition: any = null;
  private isSupported: boolean = false;
  private isAvailable: boolean = false;
  private browserInfo: BrowserInfo;

  constructor(browserInfo: BrowserInfo) {
    this.browserInfo = browserInfo;
    this.isSupported = isSpeechRecognitionSupported();
    
    if (this.isSupported) {
      const SpeechRecognition = window.SpeechRecognition || 
                                window.webkitSpeechRecognition;
      try {
        this.recognition = new SpeechRecognition();
        this.isAvailable = true;
        console.log(`Initialized for ${browserInfo.browserName}`);
      } catch (error) {
        console.error('Failed to initialize:', error);
        this.isAvailable = false;
      }
    }
  }

  createInstance() {
    if (!this.isAvailable) {
      console.error('Speech recognition not available');
      return null;
    }
    
    const SpeechRecognition = window.SpeechRecognition || 
                              window.webkitSpeechRecognition;
    const recognition = new SpeechRecognition();
    
    // Apply browser-specific configurations
    if (this.browserInfo.isBrave) {
      recognition.continuous = false;
      recognition.interimResults = false;
      recognition.lang = 'en-US';
      recognition.maxAlternatives = 1;
    } else if (this.browserInfo.isChrome || this.browserInfo.isEdge) {
      recognition.continuous = false;
      recognition.interimResults = true;
      recognition.lang = 'en-US';
      recognition.maxAlternatives = 1;
    } else if (this.browserInfo.isFirefox) {
      recognition.continuous = false;
      recognition.interimResults = false;
      recognition.lang = 'en-US';
      recognition.maxAlternatives = 1;
    } else if (this.browserInfo.isSafari) {
      recognition.continuous = false;
      recognition.interimResults = true;
      recognition.lang = 'en-US';
      recognition.maxAlternatives = 1;
    }
    
    return recognition;
  }

  isRecognitionAvailable() {
    return this.isAvailable;
  }

  getBrowserInfo() {
    return this.browserInfo;
  }
}
```

#### Browser Configurations

```typescript
// Brave Browser Configuration
{
  continuous: false,          // Single utterance mode
  interimResults: false,      // Disabled for stability
  lang: 'en-US',             // English US
  maxAlternatives: 1,        // Single best result
  audioConstraints: {
    sampleRate: 16000        // Explicit sample rate
  }
}

// Chrome/Edge Configuration
{
  continuous: false,
  interimResults: true,       // Enabled for better UX
  lang: 'en-US',
  maxAlternatives: 1
}

// Firefox Configuration
{
  continuous: false,
  interimResults: false,      // Limited support
  lang: 'en-US',
  maxAlternatives: 1
}

// Safari Configuration
{
  continuous: false,
  interimResults: true,
  lang: 'en-US',
  maxAlternatives: 1
}
```

### 3. Speech Recognition Lifecycle

#### Start Listening

```typescript
const startListening = () => {
  // Validate microphone permission
  if (!microphoneGranted) {
    toast.error('Microphone permission required.');
    return;
  }

  // Reset state for new session
  browserRestartRef.current = 0;
  sessionTranscriptRef.current = "";
  lastSpokenAtRef.current = null;
  explicitStopRef.current = false;
  setNetworkRetryCount(0);
  
  // Create recognition instance
  const recognition = recognitionManager.current?.createInstance();
  
  if (!recognition) {
    handleRecognitionFallback();
    return;
  }

  recognitionRef.current = recognition;

  // Attach event handlers
  recognition.onstart = handleStart;
  recognition.onresult = handleResult;
  recognition.onerror = handleError;
  recognition.onend = handleEnd;

  // Start recognition
  try {
    recognition.start();
    console.log('Speech recognition started');
  } catch (error) {
    console.error('Failed to start recognition:', error);
    handleRecognitionFallback();
  }
};
```

#### Handle Start

```typescript
recognition.onstart = () => {
  console.log('Speech recognition started');
  setIsListening(true);
  setIsMicOn(true);
  lastSpokenAtRef.current = Date.now();
  
  // Start silence checker
  silenceCheckerRef.current = setInterval(() => {
    checkSilence();
  }, 1000);
};
```

#### Handle Results

```typescript
recognition.onresult = (event: any) => {
  // Reset silence timer on ANY result
  lastSpokenAtRef.current = Date.now();
  
  let finalTranscript = "";
  
  // Process results based on browser
  if (browserInfo.isBrave) {
    // Brave: Only process final results
    for (let i = event.resultIndex; i < event.results.length; i++) {
      if (event.results[i].isFinal) {
        finalTranscript += event.results[i][0].transcript + " ";
      }
    }
  } else {
    // Other browsers: Process both interim and final
    for (let i = event.resultIndex; i < event.results.length; i++) {
      const transcript = event.results[i][0].transcript;
      if (event.results[i].isFinal) {
        finalTranscript += transcript + " ";
      } else {
        // Log interim results for debugging
        console.log('Interim result:', transcript);
      }
    }
  }

  // Accumulate session transcript
  if (finalTranscript.trim()) {
    sessionTranscriptRef.current += finalTranscript;
    console.log('Session transcript updated:', sessionTranscriptRef.current);
  }
};
```

#### Handle Errors

```typescript
recognition.onerror = (event: any) => {
  const errorType = event?.error || 'unknown';
  const errorMessage = event?.message || 'No error message';
  
  console.error("Speech recognition error:", {
    type: errorType,
    message: errorMessage,
    browser: browserInfo.browserName,
    timestamp: new Date().toISOString()
  });
  
  // Network error handling
  if (errorType === 'network') {
    handleNetworkError();
    return;
  }
  
  // Permission error handling
  if (errorType === 'not-allowed') {
    handlePermissionError();
    return;
  }
  
  // No-speech error handling
  if (errorType === 'no-speech') {
    handleNoSpeechError();
    return;
  }
  
  // Aborted error (intentional stop)
  if (errorType === 'aborted') {
    console.log('Recognition aborted (intentional)');
    explicitStopRef.current = true;
    return;
  }
  
  // Other errors
  console.error(`Unhandled error: ${errorType}`);
  toast.error(`Speech recognition error: ${errorType}`);
  explicitStopRef.current = true;
  recognition.stop();
};
```

#### Handle End

```typescript
recognition.onend = () => {
  console.log('Speech recognition ended');
  
  // Clear silence checker
  if (silenceCheckerRef.current) {
    clearInterval(silenceCheckerRef.current);
    silenceCheckerRef.current = null;
  }

  setIsListening(false);
  setIsMicOn(false);

  // Determine if it was true silence
  const now = Date.now();
  const silenceThreshold = browserInfo.isBrave ? 10000 : 8000;
  const wasTrueSilence = explicitStopRef.current || 
    (lastSpokenAtRef.current !== null && 
     (now - lastSpokenAtRef.current) >= silenceThreshold);
  
  // Submit accumulated transcript
  if (wasTrueSilence || sessionTranscriptRef.current.trim()) {
    const finalAnswer = sessionTranscriptRef.current.trim();
    if (finalAnswer) {
      handleAnswer(finalAnswer);
    } else {
      handleAnswer("No answer detected.");
    }
  }
};
```

### 4. Silence Detection System

#### Continuous Checker

```typescript
const checkSilence = () => {
  if (!lastSpokenAtRef.current) return;
  
  const now = Date.now();
  const silenceDuration = now - lastSpokenAtRef.current;
  
  // Browser-specific thresholds
  const threshold = browserInfo.isBrave ? 10000 : 8000;
  
  if (silenceDuration >= threshold) {
    console.log(`Silence detected: ${silenceDuration}ms`);
    explicitStopRef.current = true;
    
    try {
      recognitionRef.current?.stop();
    } catch (e) {
      console.warn('Error stopping recognition:', e);
    }
  }
};

// Start checker on recognition start
silenceCheckerRef.current = setInterval(checkSilence, 1000);

// Clear checker on recognition end
clearInterval(silenceCheckerRef.current);
silenceCheckerRef.current = null;
```

#### Timestamp Management

```typescript
// Initialize on start
lastSpokenAtRef.current = Date.now();

// Reset on ANY speech result (including interim)
recognition.onresult = (event) => {
  lastSpokenAtRef.current = Date.now();
  // ... process results
};

// Check on end
recognition.onend = () => {
  const silenceDuration = Date.now() - lastSpokenAtRef.current;
  const wasSilent = silenceDuration >= threshold;
  // ... handle accordingly
};
```

#### Silence Thresholds

| Browser | Threshold | Reason |
|---------|-----------|--------|
| Brave | 10 seconds | More conservative for stability |
| Chrome | 8 seconds | Standard threshold |
| Edge | 8 seconds | Standard threshold |
| Firefox | 8 seconds | Standard threshold |
| Safari | 8 seconds | Standard threshold |

### 5. Session Transcript Management

#### Accumulation Strategy

```typescript
// Initialize on listening start
sessionTranscriptRef.current = "";

// Accumulate on results
recognition.onresult = (event) => {
  let finalTranscript = "";
  
  for (let i = event.resultIndex; i < event.results.length; i++) {
    if (event.results[i].isFinal) {
      finalTranscript += event.results[i][0].transcript + " ";
    }
  }
  
  if (finalTranscript.trim()) {
    sessionTranscriptRef.current += finalTranscript;
  }
};

// Submit on recognition end
recognition.onend = () => {
  const finalAnswer = sessionTranscriptRef.current.trim();
  if (finalAnswer) {
    handleAnswer(finalAnswer);
  }
};
```

#### Why Session Transcript?

**Problem**: Speech recognition can stop and restart multiple times during a single answer, causing fragmented transcripts.

**Solution**: Accumulate all final results in a session transcript and submit the complete answer when recognition ends.

**Benefits**:
- Complete answers even with multiple recognition restarts
- Better handling of long responses
- Improved accuracy with accumulated context

### 6. Error Handling System

#### Network Error Recovery

```typescript
const handleNetworkError = () => {
  networkErrorCount++;
  setNetworkRetryCount(networkErrorCount);
  
  console.log(`Network error (attempt ${networkErrorCount}/3)`);
  
  if (networkErrorCount < 3) {
    // Exponential backoff
    const backoffDelay = 500 * networkErrorCount;
    
    toast.loading(`Reconnecting... (${networkErrorCount}/3)`, {
      duration: backoffDelay + 500
    });
    
    setTimeout(() => {
      try {
        const newRecognition = recognitionManager.current?.createInstance();
        if (newRecognition) {
          recognitionRef.current = newRecognition;
          setupRecognitionHandlers(newRecognition);
          newRecognition.start();
          console.log(`Restarted after ${backoffDelay}ms`);
        }
      } catch (e) {
        console.error("Failed to restart:", e);
        handleRecognitionFallback();
      }
    }, backoffDelay);
  } else {
    // After 3 attempts, switch to fallback
    console.log('Retry limit reached, switching to fallback');
    
    if (browserInfo.isBrave) {
      toast((t) => (
        <div className="space-y-2">
          <p className="font-semibold">üõ°Ô∏è Voice Failed in Brave</p>
          <p className="text-sm">After 3 attempts, voice is unavailable.</p>
          <p className="text-sm font-medium">Use text input or Retry.</p>
        </div>
      ), { duration: 6000 });
    } else {
      toast.error('Voice unavailable. Switching to text input.', {
        duration: 4000
      });
    }
    
    handleRecognitionFallback();
  }
};
```

**Retry Strategy**:
- Attempt 1: 500ms delay
- Attempt 2: 1000ms delay
- Attempt 3: 1500ms delay
- After 3 attempts: Automatic fallback

#### Permission Error Handling

```typescript
const handlePermissionError = () => {
  console.error('Microphone permission denied');
  setRecognitionAvailable(false);
  
  const message = browserInfo.isBrave
    ? 'Microphone denied. Disable Brave Shields and allow microphone.'
    : 'Microphone denied. Allow microphone in browser settings.';
  
  toast.error(message, { duration: 7000 });
  setApiError(message);
  
  explicitStopRef.current = true;
  try { 
    recognition.stop(); 
  } catch (e) { 
    /* ignore */ 
  }
};
```

#### No-Speech Error Handling

```typescript
const handleNoSpeechError = () => {
  console.log('No speech detected, moving to next question');
  explicitStopRef.current = true;
  
  try { 
    recognition.stop(); 
  } catch (e) { 
    /* ignore */ 
  }
  
  // Submit "no answer detected" to move forward
  handleAnswer("No answer detected.");
};
```

**Backend Handling**:
```python
if last_input.lower().strip() == "no answer detected.":
    response = "I did not catch your answer, so let's move ahead.\n\n"
    response += next_question
    return response_with_next_question()
```

### 7. Fallback Mode

#### Activation

```typescript
const handleRecognitionFallback = () => {
  setUseFallback(true);
  setShowManualInput(true);
  
  toast.error('Voice recognition unavailable. Use text input.', {
    duration: 4000
  });
};
```

#### Manual Input UI

```typescript
{showManualInput && (
  <div className="manual-input-container">
    <textarea
      value={manualInputText}
      onChange={(e) => setManualInputText(e.target.value)}
      onKeyDown={(e) => {
        if (e.key === 'Enter' && !e.shiftKey) {
          e.preventDefault();
          handleManualSubmit();
        }
      }}
      placeholder="Type your answer here..."
      className="w-full p-3 border rounded-lg"
      rows={4}
    />
    <button
      onClick={handleManualSubmit}
      className="btn-primary"
    >
      Send
    </button>
  </div>
)}
```

#### Manual Submit

```typescript
const handleManualSubmit = () => {
  const text = manualInputText.trim();
  
  if (!text) {
    toast.error('Please type an answer');
    return;
  }
  
  // Submit as regular answer
  handleAnswer(text);
  
  // Clear input
  setManualInputText("");
  
  // Hide manual input for next question
  setShowManualInput(false);
};
```

#### Keyboard Shortcuts

- **Enter**: Submit answer
- **Shift+Enter**: New line in textarea

### 8. Text-to-Speech (TTS)

#### Implementation

```typescript
const speak = (text: string) => {
  if (interviewEnded || !microphoneGranted) return;
  
  // Cancel any ongoing speech
  window.speechSynthesis.cancel();
  
  // Create utterance
  const utterance = new SpeechSynthesisUtterance(text);
  utterance.lang = "en-US";
  utterance.rate = browserInfo.isBrave ? 1.0 : 1.05;
  
  // Handle completion
  utterance.onend = () => {
    if (!interviewEnded && !endPending && microphoneGranted) {
      // Delay before starting recognition
      setTimeout(() => {
        startListening();
      }, browserInfo.isBrave ? 1200 : 800);
    }
  };
  
  // Handle errors
  utterance.onerror = (error) => {
    console.error('TTS error:', error);
    if (!interviewEnded && !endPending && microphoneGranted) {
      setTimeout(() => {
        startListening();
      }, browserInfo.isBrave ? 1200 : 800);
    }
  };
  
  // Speak
  window.speechSynthesis.speak(utterance);
};
```

#### TTS Configuration

| Browser | Speech Rate | Delay After TTS | Reason |
|---------|-------------|-----------------|--------|
| Brave | 1.0 | 1200ms | Prevent echo, ensure completion |
| Chrome | 1.05 | 800ms | Standard settings |
| Edge | 1.05 | 800ms | Standard settings |
| Firefox | 1.05 | 800ms | Standard settings |
| Safari | 1.05 | 800ms | Standard settings |

### 9. Resource Cleanup

#### Cleanup Function

```typescript
const cleanup = () => {
  console.log('Cleaning up voice agent resources');
  
  // 1. Stop speech recognition
  try {
    if (recognitionRef.current) {
      if (recognitionRef.current.abort) {
        recognitionRef.current.abort();
      }
      recognitionRef.current.stop();
    }
  } catch (e) {
    console.warn('Error stopping recognition:', e);
  }
  
  // 2. Cancel all TTS utterances
  try {
    if (window.speechSynthesis) {
      window.speechSynthesis.cancel();
    }
  } catch (e) {
    console.warn('Error cancelling TTS:', e);
  }
  
  // 3. Clear all timers and intervals
  try {
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      clearInterval(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    
    if (silenceCheckerRef.current) {
      clearInterval(silenceCheckerRef.current);
      silenceCheckerRef.current = null;
    }
  } catch (e) {
    console.warn('Error clearing timers:', e);
  }
  
  // 4. Null all references (prevent memory leaks)
  try {
    recognitionRef.current = null;
    lastSpokenAtRef.current = null;
    explicitStopRef.current = false;
    sessionTranscriptRef.current = "";
    browserRestartRef.current = 0;
  } catch (e) {
    console.warn('Error nulling references:', e);
  }
  
  console.log('Cleanup complete');
};
```

#### When to Cleanup

```typescript
// On component unmount
useEffect(() => {
  return () => {
    cleanup();
  };
}, []);

// On interview end
const endInterview = async () => {
  // ... save transcript and feedback
  cleanup();
  router.push('/dashboard');
};

// On page unload/refresh
useEffect(() => {
  const handleBeforeUnload = () => {
    cleanup();
  };
  
  window.addEventListener('beforeunload', handleBeforeUnload);
  
  return () => {
    window.removeEventListener('beforeunload', handleBeforeUnload);
  };
}, []);
```

## üé® UI Integration

### Status Indicators

```typescript
// Listening state
{isListening && (
  <div className="flex items-center gap-2 text-green-500">
    <Mic className="w-5 h-5 animate-pulse" />
    <span>Listening...</span>
  </div>
)}

// Processing state
{agentThinking && (
  <div className="flex items-center gap-2 text-blue-500">
    <Spinner className="w-5 h-5 animate-spin" />
    <span>AI is processing...</span>
  </div>
)}

// Reconnecting state
{networkRetryCount > 0 && (
  <div className="flex items-center gap-2 text-yellow-500">
    <RefreshCw className="w-5 h-5 animate-spin" />
    <span>Reconnecting... ({networkRetryCount}/3)</span>
  </div>
)}

// Fallback mode
{useFallback && (
  <div className="flex items-center gap-2 text-gray-500">
    <Keyboard className="w-5 h-5" />
    <span>Type your answer...</span>
  </div>
)}

// Error state
{apiError && (
  <div className="flex items-center gap-2 text-red-500">
    <AlertCircle className="w-5 h-5" />
    <span>{apiError}</span>
  </div>
)}
```

### Avatar Styling

```typescript
<div className={cn(
  "avatar-container rounded-full p-4 transition-all",
  isMicOn && "ring-2 ring-white ring-offset-2"
)}>
  <Bot className="w-8 h-8" />
</div>
```

### Brave Browser Badge

```typescript
{browserInfo.isBrave && (
  <div className="flex items-center gap-2 px-3 py-1 bg-orange-100 rounded-full">
    <span>üõ°Ô∏è</span>
    <span className="text-sm font-medium">Brave Browser</span>
    <AlertCircle className="w-4 h-4 text-orange-600" />
  </div>
)}
```

## üìä Performance Metrics

### Latency Targets

| Operation | Target | Actual |
|-----------|--------|--------|
| Recognition Start | < 500ms | ~300ms |
| Result Processing | < 100ms | ~50ms |
| Silence Detection | 1s intervals | 1s |
| TTS Start | < 200ms | ~150ms |
| Fallback Switch | < 500ms | ~300ms |

### Browser Performance

| Browser | Recognition Quality | Stability | Latency |
|---------|-------------------|-----------|---------|
| Chrome | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ~300ms |
| Edge | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ~300ms |
| Brave | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ~400ms |
| Firefox | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ~350ms |
| Safari | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ~350ms |

## üîç Debugging

### Enable Debug Logging

```typescript
// Add to component
const DEBUG = process.env.NODE_ENV === 'development';

if (DEBUG) {
  console.log('Voice Agent Debug Info:', {
    browser: browserInfo.browserName,
    isListening,
    isMicOn,
    sessionTranscript: sessionTranscriptRef.current,
    lastSpokenAt: lastSpokenAtRef.current,
    networkRetryCount,
    useFallback
  });
}
```

### Browser Console Commands

```javascript
// Check speech recognition support
console.log('Speech Recognition:', 
  'SpeechRecognition' in window || 
  'webkitSpeechRecognition' in window
);

// Check microphone devices
navigator.mediaDevices.enumerateDevices()
  .then(devices => {
    const mics = devices.filter(d => d.kind === 'audioinput');
    console.log('Microphones:', mics);
  });

// Test microphone access
navigator.mediaDevices.getUserMedia({ audio: true })
  .then(stream => {
    console.log('Microphone working!');
    stream.getTracks().forEach(track => track.stop());
  })
  .catch(err => console.error('Microphone error:', err));
```

## üöÄ Best Practices

### 1. Always Check Permissions First

```typescript
// Request permission before starting interview
const requestPermission = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    stream.getTracks().forEach(track => track.stop());
    return true;
  } catch (error) {
    return false;
  }
};
```

### 2. Handle All Error Types

```typescript
// Comprehensive error handling
recognition.onerror = (event) => {
  switch (event.error) {
    case 'network':
      handleNetworkError();
      break;
    case 'not-allowed':
      handlePermissionError();
      break;
    case 'no-speech':
      handleNoSpeechError();
      break;
    case 'aborted':
      // Intentional stop, no action needed
      break;
    default:
      handleUnknownError(event.error);
  }
};
```

### 3. Always Cleanup Resources

```typescript
// Cleanup on unmount and interview end
useEffect(() => {
  return () => cleanup();
}, []);
```

### 4. Provide Visual Feedback

```typescript
// Always show current state to user
{isListening && <MicIcon />}
{agentThinking && <SpinnerIcon />}
{networkRetryCount > 0 && <RetryIcon />}
{useFallback && <KeyboardIcon />}
```

### 5. Test Across Browsers

```bash
# Test checklist
- [ ] Chrome (latest)
- [ ] Edge (latest)
- [ ] Firefox (latest)
- [ ] Safari (latest)
- [ ] Brave (latest)
- [ ] Mobile browsers
```

## üìö Additional Resources

- [Web Speech API Documentation](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API)
- [SpeechRecognition Interface](https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition)
- [SpeechSynthesis Interface](https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesis)
- [VOICE_INTERVIEW_TROUBLESHOOTING.md](./VOICE_INTERVIEW_TROUBLESHOOTING.md)
- [INTERVIEW_FLOW.md](./INTERVIEW_FLOW.md)

---

**Last Updated**: November 2025  
**Maintained By**: Zerko Development Team
